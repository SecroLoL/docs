# Introduction 

## Quick Summary 

`Scorer`s act as measurement tools for evaluating LLM systems based on specific criteria. `judgeval` comes with a set of 10+ built-in scorers that you can easily start with, including:
- Answer Relevancy
- Contextual Precision
- Contextual Recall
- Contextual Relevancy
- Faithfulness
- Hallucination
- Summarization
- Tool Correctness
- JSON Correctness
- Custom Scorers
- Classifier Scorers

`Tip`: 

We're always building new scorers to add to `judgeval`. If you have a specific scorer in mind, please let us know at `contact@judment.ai`!

`Scorer`s execute on `Example`s, `GroundTruthExample`s, and `EvalDataset`s and produce a score between 0 and 1. 
As such, you can set a `threshold` to determine whether an evaluation was successful or not. 
A default scorer will only succeed if the score is greater than or equal to the `threshold`. 

## Categories of Scorers
`judgeval` supports three categories of scorers. 
- Default Scorers: built-in scorers that are ready to use
- Custom Scorers: Versatile and powerful scorers that you can tailor to your own evaluation needs
- Classifier Scorers: A special custom scorer that can evaluate your LLM system using a natural language criteria

In this section, we'll cover each kind of scorer and how to use them.

### Default Scorers
Most of the ready-to-use scorers in `judgeval` are LLM judges, meaning they use LLMs to evaluate your LLM system. This is intentional since LLM evaluations are flexible, scalable, and strongly correlate with human evaluation.

`judgeval`'s default scorers have been crafted by our research team based on leading work in the LLM evaluation community. 
You can access our scorer implementations via `judgeval` which run the scorers on Judgment's infrastructure. 
Our implementations are described on their respective documentation pages. Judgment implementations of default scorers are backed by leading industry/academic research and are preferable to other implementations because:
- They are meticulously prompt-engineered to maximize evaluation quality and consistency
- Provide a chain of thought for evaluation scores, so you can double-check the evaluation quality
- Can be run using any LLM, including Judgment's **state-of-the-art LLM judges** developed in collaboration with **Stanford's AI Lab**.

### Custom Scorers

If you find that none of the default scorers meet your evaluation needs, setting up a custom scorer is easy with `judgeval`.
You can create a custom scorer by inheritng from the `CustomScorer` class and implementing three methods:
- `score_example()`: produces a score for a single `Example`.
- `a_score_example()`: async version of `score_example()`. You may use the same implementation logic as `score_example()`.
- `success_check()`: determines whether an evaluation was successful.

Custom scorers can be as simple or complex as you want, and do not need to use LLMs. For sample implementations, check out the `CustomScorer` documentation page. TODO add link here


### Classifier Scorers

Classifier scorers are a special type of custom scorer that can evaluate your LLM system using a natural language criteria. 

TODO update this section when SDK is updated

## Running Scorers

All scorers in `judgeval` can be run uniformly through the `JudgmentClient`. All scorers are set to run in async mode by default in order to support parallelized evaluations for large datasets.

```
...

client = JudgmentClient()
results = client.run_evaluation(
    examples=[example],
    scorers=[scorer],
    model="gpt-4o-mini",
)
```

If you want to execute a `CustomScorer` without running it through the `JudgmentClient`, you can score locally.
Simply use the `score_example()` or `a_score_example()` method directly:

```
...

example = Example(input="...", actual_output="...")

scorer = CustomScorer()  # Your scorer here
score = scorer.score_example(example)
```

`Tip`:

To learn about how a certain default scorer works, check out its documentation page for a deep dive into how scores are calculated and what fields are required.
